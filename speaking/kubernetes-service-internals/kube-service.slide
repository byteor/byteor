Kubernetes Service Internals
成都
22 Jun 2020

周朋
ideapark@petalmail.com
https://github.com/ideapark
微信 ideaparkio

* License

Copyright © Park Zhou <ideapark@petalmail.com>

* Key Takeaways

- Local Multi-Node Cluster
- Kubernetes Service Internals

.link https://kubernetes.io/docs/concepts/services-networking/service Kubernetes Service

- How Kubernetes Controllers Work

* Kubernetes Cluster

- $ kind create cluster --config=kind.yaml

  $ kubectl get nodes -o wide
  NAME                 STATUS   ROLES    AGE     VERSION   INTERNAL-IP
  kind-control-plane   Ready    master   10m     v1.18.2   172.18.0.4
  kind-worker          Ready    <none>   9m38s   v1.18.2   172.18.0.3
  kind-worker2         Ready    <none>   9m38s   v1.18.2   172.18.0.2

.link kind.yaml
.link https://kind.sigs.k8s.io Kind

- Cluster Networking

  +----------------------+----------------------------------+
  | NAME                 | SUBNET CIDR                      |
  +----------------------+----------------------------------+
  | Node                 | 172.18.0.0/16                    |
  | Service              | 10.96.0.0/12                     |
  | Pod                  | 10.244.0.0/16                    |
  +----------------------+----------------------------------+

* Cluster Overview

  $ kubectl -n kube-system get pods -o wide --sort-by=spec.nodeName
  NAME                                         READY   STATUS     IP           NODE
  coredns-66bff467f8-7zvcz                     1/1     Running    10.244.0.4   kind-control-plane
  coredns-66bff467f8-h9mts                     1/1     Running    10.244.0.2   kind-control-plane
  etcd-kind-control-plane                      1/1     Running    172.18.0.4   kind-control-plane
  kube-proxy-ftgtj                             1/1     Running    172.18.0.4   kind-control-plane
  kindnet-gkql5                                1/1     Running    172.18.0.4   kind-control-plane
  kube-scheduler-kind-control-plane            1/1     Running    172.18.0.4   kind-control-plane
  kube-apiserver-kind-control-plane            1/1     Running    172.18.0.4   kind-control-plane
  kube-controller-manager-kind-control-plane   1/1     Running    172.18.0.4   kind-control-plane
  kindnet-79zhl                                1/1     Running    172.18.0.3   kind-worker
  kube-proxy-q8fr9                             1/1     Running    172.18.0.3   kind-worker
  kube-proxy-c5qk5                             1/1     Running    172.18.0.2   kind-worker2
  kindnet-s4wtx                                1/1     Running    172.18.0.2   kind-worker2

  $ kubectl get services -o wide
  NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
  kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   57s   <none>

* Cluster Node Components

  +-----------+--------------------+      +-----------+--------------------+
  | HOSTNAME: | kind-worker        |      | HOSTNAME: | kind-worker2       |  #worker: 1 2 3 ... N
  +-----------+--------------------+      +-----------+--------------------+
  | SYSTEMD:  | docker kubelet     |      | SYSTEMD:  | docker kubelet     |
  |-----------+--------------------|      |-----------+--------------------|
  | POD:      | kube-proxy kindnet |      | POD:      | kube-proxy kindnet |
  +-----------+--------------------+      +-----------+--------------------+

                   +------------+-------------------------------+
                   | HOSTNAME:  | kind-control-plane            |   #control-plane: 1 3 (5 7)
                   +------------+-------------------------------+
                   | SYSTEMD:   | docker kubelet                |
                   |------------+-------------------------------|
                   | STATIC POD:| kube-apiserver kube-scheduler |
                   |            | kube-controller-manager etcd  |
                   |------------+-------------------------------|
                   | POD:       | kube-proxy    coredns-5pt4d   |
                   |            | kindnet       coredns-6x42l   |
                   +------------+-------------------------------+

* Kuard App

  +-----------------------------------------------------------------------------------------------+
  |      Name: kuard                                                                              |
  | Namespace: default                                                                            |
  |    Labels: app=kuard                                                                          |
  |  Selector: app=kuard                                                                          |
  |  Replicas: 2                                                                                  |
  |     Image: gcr.io/kuar-demo/kuard-amd64:blue                                                  |
  |      Port: 8080                                                                               |
  |  Liveness: http-get http://:8080/healthy delay=0s timeout=1s period=10s #success=1 #failure=3 |
  | Readiness: http-get http://:8080/ready   delay=0s timeout=1s period=10s #success=1 #failure=3 |
  +-----------------------------------------------------------------------------------------------+

.link kuard.yaml

  $ kubectl apply -f kuard.yaml

  # in-cluster testing pod
  $ kubectl run dnsutils --image=tutum/dnsutils --command -- sleep infinity

* Cluster Node Components (Kuard App Running)

  +-----------+--------------------+      +-----------+--------------------+
  | HOSTNAME: | kind-worker        |      | HOSTNAME: | kind-worker2       |  #worker: 1 2 3 ... N
  +-----------+--------------------+      +-----------+--------------------+
  | SYSTEMD:  | docker kubelet     |      | SYSTEMD:  | docker kubelet     |
  |-----------+--------------------|      |-----------+--------------------|
  | POD:      | kube-proxy kindnet |      | POD:      | kube-proxy kindnet |
  |           | [kuard] [dnsutils] |      |           | [kuard]            |
  +-----------+--------------------+      +-----------+--------------------+

                   +------------+-------------------------------+
                   | HOSTNAME:  | kind-control-plane            |   #control-plane: 1 3 (5 7)
                   +------------+-------------------------------+
                   | SYSTEMD:   | docker kubelet                |
                   |------------+-------------------------------|
                   | STATIC POD:| kube-apiserver kube-scheduler |
                   |            | kube-controller-manager etcd  |
                   |------------+-------------------------------|
                   | POD:       | kube-proxy    coredns-5pt4d   |
                   |            | kindnet       coredns-6x42l   |
                   +------------+-------------------------------+

* kube-proxy

The Kubernetes network proxy runs on each node (DaemonSet). This reflects
services as defined in the Kubernetes API on each node and can do simple TCP,
UDP, and SCTP stream forwarding or round robin TCP, UDP, and SCTP forwarding
across a set of backends (Endpoints).

  $ kubectl -n kube-system edit daemonset/kube-proxy
  $ kubectl -n kube-system logs -f daemonset/kube-proxy

- kube-proxy verbose mode

  spec:
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - -v=6
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)

* Service & Endpoints

Endpoints is a collection of endpoints that implement the actual service.
service is name and ip is stable identifier, otherwise endpoints is on the
going.

- The very first default in-cluster service named kubernetes which resides in namespace default.

  $ kubectl get services -o wide

  +-------------------------------------------------+
  |       Name: kubernetes                          |
  |  Namespace: default                             |
  |     Labels: component=apiserver                 |
  |             provider=kubernetes                 |
  |       Type: ClusterIP                           |
  |         IP: 10.96.0.1                           |
  |       Port: https  443/TCP                      |
  | TargetPort: 6443/TCP                            |
  |  Endpoints: 172.18.0.2:6443                     |
  +-------------------------------------------------+

* Pod Readiness Probe

The kubelet uses readiness probes to know when a container is ready to start
accepting traffic. A Pod is considered ready when all of its containers are
ready. One use of this signal is to control which Pods are used as backends (aka
Endpoints) for Services. When a Pod is not ready, it is removed from Service
load balancers.

  # Make kuard app accessible out side of cluster temporarily
  # we will see how readiness probe affect service's endpoints
  $ kubectl port-forward deployment/kuard 9090:8080

.link http://localhost:9090 Kuard

* Service (in-cluster)

- ClusterIP

  $ kubectl expose deployment/kuard --port=80 --target-port=8080 --type=ClusterIP
  $ kubectl describe service/kuard

  # endpoints is on the going
  $ kubectl describe endpoints/kuard

  # iptables rules
  $ iptables -t nat -S

- Cluster Networking

  +----------------------+----------------------------------+
  | NAME                 | SUBNET CIDR                      |
  +----------------------+----------------------------------+
  | Node                 | 172.18.0.0/16                    |
  | Service              | 10.96.0.0/12                     |
  | Pod                  | 10.244.0.0/16                    |
  +----------------------+----------------------------------+

* Service (out-of-cluster)

- NodePort

  kubectl expose deployment/kuard --port=80 --target-port=8080 --type=NodePort
  kubectl describe service/kuard

  # endpoints is on the going
  $ kubectl describe endpoints/kuard

  # iptables rules
  $ iptables -t nat -S

  # listening port
  $ ss -4tlnp

- Cluster Networking

  +----------------------+----------------------------------+
  | NAME                 | SUBNET CIDR                      |
  +----------------------+----------------------------------+
  | Node                 | 172.18.0.0/16                    |
  | Service              | 10.96.0.0/12                     |
  | Pod                  | 10.244.0.0/16                    |
  +----------------------+----------------------------------+

* Service (out-of-cluster with cloud vendor)

- LoadBalancer

  $ kubectl expose deployment/kuard --port=80 --target-port=8080 --type=LoadBalancer
  $ kubectl describe service/kuard

  # endpoints is one the going
  $ kubectl describe endpoints/kuard

  # iptables rules
  $ iptables -t nat -S

  # listening port
  $ ss -4tlnp

- Cluster Networking

  +----------------------+----------------------------------+
  | NAME                 | SUBNET CIDR                      |
  +----------------------+----------------------------------+
  | Node                 | 172.18.0.0/16                    |
  | Service              | 10.96.0.0/12                     |
  | Pod                  | 10.244.0.0/16                    |
  +----------------------+----------------------------------+

* FAQ

- Why ping service ip not working?
- DNS or Environment Variables service discovering, how and why?
- NAT limitations and bugs.

.link https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/#motivation nf_conntrack_udp_timeout

- When node offline (e,g. power off), what will happen?
- Why there is another EndpointSlice?
