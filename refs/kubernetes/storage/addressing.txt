# 寻址流程

## File

此处的file就是用户需要存储或者访问的文件。对于一个基于Ceph开发的对象存储应用而言，
这个file也就对应于应用中的“对象”，也就是用户直接操作的“对象”。

## Ojbect

此处的object是RADOS所看到的“对象”。Object与上面提到的file的区别是，object的最
大size由RADOS限定（通常为2MB或4MB），以便实现底层存储的组织管理。因此，当上层应
用向RADOS存入size很大的file时，需要将file切分成统一大小的一系列object（最后一个
的大小可以不同）进行存储。为避免混淆，在本文中将尽量避免使用中文的“对象”这一名
词，而直接使用file或object进行说明。

## PG（Placement Group）

顾名思义，PG的用途是对object的存储进行组织和位置映射。具体而言，一个PG负责组织若
干个object（可以为数千个甚至更多），但一个object只能被映射到一个PG中，即，PG和
object之间是“一对多”映射关系。同时，一个PG会被映射到n个OSD上，而每个OSD上都会
承载大量的PG，即，PG和OSD之间是“多对多”映射关系。在实践当中，n至少为2，如果用
于生产环境，则至少为3。一个OSD上的PG则可达到数百个。事实上，PG数量的设置牵扯到数
据分布的均匀性问题。

## OSD

即object storage device，前文已经详细介绍，此处不再展开。唯一需要说明的是，OSD的
数量事实上也关系到系统的数据分布均匀性，因此其数量不应太少。在实践当中，至少也应
该是数十上百个的量级才有助于Ceph系统的设计发挥其应有的优势。

## Failure domain

这个概念在论文中并没有进行定义，好在对分布式存储系统有一定概念的读者应该能够了解
其大意。

基于上述定义，便可以对寻址流程进行解释了。具体而言，Ceph中的寻址至少要经历以下三次映射：

（1）File -> object映射

这次映射的目的是，将用户要操作的file，映射为RADOS能够处理的object。其映射十分简
单，本质上就是按照object的最大size对file进行切分，相当于RAID中的条带化过程。这种
切分的好处有二：一是让大小不限的file变成最大size一致、可以被RADOS高效管理的
object；二是让对单一file实施的串行处理变为对多个object实施的并行化处理。

每一个切分后产生的object将获得唯一的oid，即object id。其产生方式也是线性映射，极
其简单。图中，ino是待操作file的元数据，可以简单理解为该file的唯一id。ono则是由该
file切分产生的某个object的序号。而oid就是将这个序号简单连缀在该file id之后得到的。
举例而言，如果一个id为filename的file被切分成了三个object，则其object序号依次为0、
1和2，而最终得到的oid就依次为filename0、filename1和filename2。

这里隐含的问题是，ino的唯一性必须得到保证，否则后续映射无法正确进行。

（2）Object -> PG映射

在file被映射为一个或多个object之后，就需要将每个object独立地映射到一个PG中去。这
个映射过程也很简单，如图中所示，其计算公式是：

hash(oid) & mask -> pgid

由此可见，其计算由两步组成。首先是使用Ceph系统指定的一个静态哈希函数计算oid的哈
希值，将oid映射成为一个近似均匀分布的伪随机值。然后，将这个伪随机值和mask按位相
与，得到最终的PG序号（pgid）。根据RADOS的设计，给定PG的总数为m（m应该为2的整数
幂），则mask的值为m-1。因此，哈希值计算和按位与操作的整体结果事实上是从所有m个PG
中近似均匀地随机选择一个。基于这一机制，当有大量object和大量PG时，RADOS能够保证
object和PG之间的近似均匀映射。又因为object是由file切分而来，大部分object的size相
同，因而，这一映射最终保证了，各个PG中存储的object的总数据量近似均匀。

从介绍不难看出，这里反复强调了“大量”。只有当object和PG的数量较多时，这种伪随机
关系的近似均匀性才能成立，Ceph的数据存储均匀性才有保证。为保证“大量”的成立，一
方面，object的最大size应该被合理配置，以使得同样数量的file能够被切分成更多的
object；另一方面，Ceph也推荐PG总数应该为OSD总数的数百倍，以保证有足够数量的PG可
供映射。

（3）PG -> OSD映射

第三次映射就是将作为object的逻辑组织单元的PG映射到数据的实际存储单元OSD。如图所
示，RADOS采用一个名为CRUSH的算法，将pgid代入其中，然后得到一组共n个OSD。这n个OSD
即共同负责存储和维护一个PG中的所有object。前已述及，n的数值可以根据实际应用中对
于可靠性的需求而配置，在生产环境下通常为3。具体到每个OSD，则由其上运行的OSD
deamon负责执行映射到本地的object在本地文件系统中的存储、访问、元数据维护等操作。

和“object -> PG”映射中采用的哈希算法不同，这个CRUSH算法的结果不是绝对不变的，
而是受到其他因素的影响。其影响因素主要有二：

一是当前系统状态，也就是在《“Ceph浅析”系列之四——逻辑结构》中曾经提及的cluster
map。当系统中的OSD状态、数量发生变化时，cluster map可能发生变化，而这种变化将会
影响到PG与OSD之间的映射。

二是存储策略配置。这里的策略主要与安全相关。利用策略配置，系统管理员可以指定承载
同一个PG的3个OSD分别位于数据中心的不同服务器乃至机架上，从而进一步改善存储的可靠
性。

因此，只有在系统状态（cluster map）和存储策略都不发生变化的时候，PG和OSD之间的映
射关系才是固定不变的。在实际使用当中，策略一经配置通常不会改变。而系统状态的改变
或者是由于设备损坏，或者是因为存储集群规模扩大。好在Ceph本身提供了对于这种变化的
自动化支持，因而，即便PG与OSD之间的映射关系发生了变化，也并不会对应用造成困扰。
事实上，Ceph正是需要有目的的利用这种动态映射关系。正是利用了CRUSH的动态特性，
Ceph可以将一个PG根据需要动态迁移到不同的OSD组合上，从而自动化地实现高可靠性、数
据分布re-blancing等特性。

之所以在此次映射中使用CRUSH算法，而不是其他哈希算法，原因之一正是CRUSH具有上述可
配置特性，可以根据管理员的配置参数决定OSD的物理位置映射策略；另一方面是因为CRUSH
具有特殊的“稳定性”，也即，当系统中加入新的OSD，导致系统规模增大时，大部分PG与
OSD之间的映射关系不会发生改变，只有少部分PG的映射关系会发生变化并引发数据迁移。
这种可配置性和稳定性都不是普通哈希算法所能提供的。因此，CRUSH算法的设计也是Ceph
的核心内容之一，具体介绍可以参考[2]。

至此为止，Ceph通过三次映射，完成了从file到object、PG和OSD整个映射过程。通观整个
过程，可以看到，这里没有任何的全局性查表操作需求。至于唯一的全局性数据结构
cluster map，在后文中将加以介绍。可以在这里指明的是，cluster map的维护和操作都是
轻量级的，不会对系统的可扩展性、性能等因素造成不良影响。

一个可能出现的困惑是：为什么需要同时设计第二次和第三次映射？难道不重复么？关于这
一点，Sage在其论文中解说不多，而笔者个人的分析如下：

我们可以反过来想像一下，如果没有PG这一层映射，又会怎么样呢？在这种情况下，一定需
要采用某种算法，将object直接映射到一组OSD上。如果这种算法是某种固定映射的哈希算
法，则意味着一个object将被固定映射在一组OSD上，当其中一个或多个OSD损坏时，object
无法被自动迁移至其他OSD上（因为映射函数不允许），当系统为了扩容新增了OSD时，
object也无法被re-balance到新的OSD上（同样因为映射函数不允许）。这些限制都违背了
Ceph系统高可靠性、高自动化的设计初衷。

如果采用一个动态算法（例如仍然采用CRUSH算法）来完成这一映射，似乎是可以避免静态
映射导致的问题。但是，其结果将是各个OSD所处理的本地元数据量爆增，由此带来的计算
复杂度和维护工作量也是难以承受的。

例如，在Ceph的现有机制中，一个OSD平时需要和与其共同承载同一个PG的其他OSD交换信息，
以确定各自是否工作正常，是否需要进行维护操作。由于一个OSD上大约承载数百个PG，每
个PG内通常有3个OSD，因此，一段时间内，一个OSD大约需要进行数百至数千次OSD信息交换。

然而，如果没有PG的存在，则一个OSD需要和与其共同承载同一个object的其他OSD交换信息。
由于每个OSD上承载的object很可能高达数百万个，因此，同样长度的一段时间内，一个OSD
大约需要进行的OSD间信息交换将暴涨至数百万乃至数千万次。而这种状态维护成本显然过
高。

综上所述，笔者认为，引入PG的好处至少有二：一方面实现了object和OSD之间的动态映射，
从而为Ceph的可靠性、自动化等特性的实现留下了空间；另一方面也有效简化了数据的存储
组织，大大降低了系统的维护管理开销。理解这一点，对于彻底理解Ceph的对象寻址机制，
是十分重要的。
