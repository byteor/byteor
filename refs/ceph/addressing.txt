# 寻址流程

## File

此处的 file 就是用户需要存储或者访问的文件。对于一个基于 Ceph 开发的对象存储应用
而言，这个 file 也就对应于应用中的 “对象”，也就是用户直接操作的 “对象”。

## Ojbect

此处的 object 是 RADOS 所看到的 “对象”。Object 与上面提到的 file 的区别是，
object 的最大 size 由 RADOS 限定（通常为 2MB 或 4MB），以便实现底层存储的组织管
理。因此，当上层应用向 RADOS 存入 size 很大的 file 时，需要将 file 切分成统一大
小的一系列 object（最后一个的大小可以不同）进行存储。为避免混淆，在本文中将尽量
避免使用中文的 “对象” 这一名词，而直接使用 file 或 object 进行说明。

## PG（Placement Group）

顾名思义，PG 的用途是对 object 的存储进行组织和位置映射。具体而言，一个 PG 负责
组织若干个 object（可以为数千个甚至更多），但一个 object 只能被映射到一个 PG 中，
即，PG 和object 之间是 “一对多” 映射关系。同时，一个 PG 会被映射到 n 个 OSD 上，
而每个 OSD 上都会承载大量的 PG，即，PG 和 OSD 之间是 “多对多” 映射关系。在实践当
中，n 至少为 2，如果用于生产环境，则至少为 3。一个 OSD 上的 PG 则可达到数百个。
事实上，PG 数量的设置牵扯到数据分布的均匀性问题。

## OSD

即 object storage device，前文已经详细介绍，此处不再展开。唯一需要说明的是，OSD
的数量事实上也关系到系统的数据分布均匀性，因此其数量不应太少。在实践当中，至少也
应该是数十上百个的量级才有助于 Ceph 系统的设计发挥其应有的优势。

## Failure domain

这个概念在论文中并没有进行定义，好在对分布式存储系统有一定概念的读者应该能够了解
其大意。

基于上述定义，便可以对寻址流程进行解释了。具体而言，Ceph 中的寻址至少要经历以下三次映射：

（1）File -> object 映射

这次映射的目的是，将用户要操作的 file，映射为 RADOS 能够处理的 object。其映射十
分简单，本质上就是按照 object 的最大 size 对 file 进行切分，相当于 RAID 中的条带
化过程。这种切分的好处有二：一是让大小不限的 file 变成最大 size 一致、可以被
RADOS 高效管理的object；二是让对单一file实施的串行处理变为对多个 object 实施的并
行化处理。

每一个切分后产生的 object 将获得唯一的 oid，即 object id。其产生方式也是线性映射，
极其简单。图中，ino 是待操作 file 的元数据，可以简单理解为该 file 的唯一 id。ono
则是由该file 切分产生的某个 object 的序号。而 oid 就是将这个序号简单连缀在该
file id 之后得到的。举例而言，如果一个 id 为 filename 的 file 被切分成了三个
object，则其 object 序号依次为 0、1 和 2，而最终得到的 oid 就依次为 filename0、
filename1 和 filename2。

这里隐含的问题是，ino 的唯一性必须得到保证，否则后续映射无法正确进行。

（2）Object -> PG 映射

在 file 被映射为一个或多个 object 之后，就需要将每个 object 独立地映射到一个 PG 中去。这
个映射过程也很简单，如图中所示，其计算公式是：

hash(oid) & mask -> pgid

由此可见，其计算由两步组成。首先是使用 Ceph 系统指定的一个静态哈希函数计算 oid
的哈希值，将 oid 映射成为一个近似均匀分布的伪随机值。然后，将这个伪随机值和 mask
按位相与，得到最终的 PG 序号（pgid）。根据 RADOS 的设计，给定 PG 的总数为 m（m
应该为 2 的整数幂），则 mask 的值为 m-1。因此，哈希值计算和按位与操作的整体结果
事实上是从所有 m 个 PG中近似均匀地随机选择一个。基于这一机制，当有大量 object 和
大量 PG 时，RADOS 能够保证object 和 PG 之间的近似均匀映射。又因为 object 是由
file 切分而来，大部分 object 的 size 相同，因而，这一映射最终保证了，各个 PG 中
存储的 object 的总数据量近似均匀。

从介绍不难看出，这里反复强调了“大量”。只有当 object 和 PG 的数量较多时，这种伪随
机关系的近似均匀性才能成立，Ceph 的数据存储均匀性才有保证。为保证 “大量” 的成立，
一方面，object 的最大 size 应该被合理配置，以使得同样数量的 file 能够被切分成更
多的 object；另一方面，Ceph 也推荐 PG 总数应该为 OSD 总数的数百倍，以保证有足够
数量的 PG 可供映射。

（3）PG -> OSD映射

第三次映射就是将作为 object 的逻辑组织单元的 PG 映射到数据的实际存储单元 OSD。如
图所示，RADOS 采用一个名为 CRUSH 的算法，将 pgid 代入其中，然后得到一组共 n 个
OSD。这 n 个 OSD即共同负责存储和维护一个 PG 中的所有 object。前已述及，n 的数值
可以根据实际应用中对于可靠性的需求而配置，在生产环境下通常为 3。具体到每个 OSD，
则由其上运行的 OSD deamon 负责执行映射到本地的 object 在本地文件系统中的存储、访
问、元数据维护等操作。

和 “object -> PG” 映射中采用的哈希算法不同，这个 CRUSH 算法的结果不是绝对不变的，
而是受到其他因素的影响。其影响因素主要有二：

一是当前系统状态，也就是在《“Ceph浅析”系列之四——逻辑结构》中曾经提及的 cluster
map。当系统中的 OSD 状态、数量发生变化时，cluster map 可能发生变化，而这种变化将
会影响到 PG 与 OSD 之间的映射。

二是存储策略配置。这里的策略主要与安全相关。利用策略配置，系统管理员可以指定承载
同一个 PG 的 3 个 OSD 分别位于数据中心的不同服务器乃至机架上，从而进一步改善存储
的可靠性。

因此，只有在系统状态（cluster map）和存储策略都不发生变化的时候，PG 和 OSD 之间
的映射关系才是固定不变的。在实际使用当中，策略一经配置通常不会改变。而系统状态的
改变或者是由于设备损坏，或者是因为存储集群规模扩大。好在 Ceph 本身提供了对于这种
变化的自动化支持，因而，即便 PG 与 OSD 之间的映射关系发生了变化，也并不会对应用
造成困扰。事实上，Ceph 正是需要有目的的利用这种动态映射关系。正是利用了 CRUSH 的
动态特性，Ceph 可以将一个 PG 根据需要动态迁移到不同的 OSD 组合上，从而自动化地实
现高可靠性、数据分布 re-blancing 等特性。

之所以在此次映射中使用 CRUSH 算法，而不是其他哈希算法，原因之一正是 CRUSH 具有上
述可配置特性，可以根据管理员的配置参数决定 OSD 的物理位置映射策略；另一方面是因
为 CRUSH具有特殊的“稳定性”，也即，当系统中加入新的 OSD，导致系统规模增大时，大部
分 PG 与OSD 之间的映射关系不会发生改变，只有少部分 PG 的映射关系会发生变化并引发
数据迁移。这种可配置性和稳定性都不是普通哈希算法所能提供的。因此，CRUSH 算法的设
计也是 Ceph的核心内容之一，具体介绍可以参考[2]。

至此为止，Ceph 通过三次映射，完成了从 file 到 object、PG 和 OSD 整个映射过程。通
观整个过程，可以看到，这里没有任何的全局性查表操作需求。至于唯一的全局性数据结构
cluster map，在后文中将加以介绍。可以在这里指明的是，cluster map 的维护和操作都
是轻量级的，不会对系统的可扩展性、性能等因素造成不良影响。

一个可能出现的困惑是：为什么需要同时设计第二次和第三次映射？ 难道不重复么？ 关于
这一点，Sage 在其论文中解说不多，而笔者个人的分析如下：

我们可以反过来想像一下，如果没有 PG 这一层映射，又会怎么样呢？在这种情况下，一定
需要采用某种算法，将 object 直接映射到一组 OSD 上。如果这种算法是某种固定映射的
哈希算法，则意味着一个 object 将被固定映射在一组 OSD 上，当其中一个或多个 OSD 损
坏时，object无法被自动迁移至其他 OSD 上（因为映射函数不允许），当系统为了扩容新
增了 OSD 时，object 也无法被 re-balance 到新的 OSD 上（同样因为映射函数不允许）。
这些限制都违背了Ceph 系统高可靠性、高自动化的设计初衷。

如果采用一个动态算法（例如仍然采用 CRUSH 算法）来完成这一映射，似乎是可以避免静
态映射导致的问题。但是，其结果将是各个 OSD 所处理的本地元数据量爆增，由此带来的
计算复杂度和维护工作量也是难以承受的。

例如，在 Ceph 的现有机制中，一个 OSD 平时需要和与其共同承载同一个 PG 的其他 OSD
交换信息，以确定各自是否工作正常，是否需要进行维护操作。由于一个 OSD 上大约承载
数百个 PG，每个 PG 内通常有 3 个 OSD，因此，一段时间内，一个 OSD 大约需要进行数
百至数千次 OSD 信息交换。

然而，如果没有 PG 的存在，则一个 OSD 需要和与其共同承载同一个 object 的其他 OSD
交换信息。由于每个 OSD 上承载的 object 很可能高达数百万个，因此，同样长度的一段
时间内，一个 OSD大约需要进行的 OSD 间信息交换将暴涨至数百万乃至数千万次。而这种
状态维护成本显然过高。

综上所述，笔者认为，引入 PG 的好处至少有二：一方面实现了 object 和 OSD 之间的动
态映射，从而为 Ceph 的可靠性、自动化等特性的实现留下了空间；另一方面也有效简化了
数据的存储组织，大大降低了系统的维护管理开销。理解这一点，对于彻底理解 Ceph 的对
象寻址机制，是十分重要的。
